# Copyright (c) 2025 MiroMind
# This source code is licensed under the MIT License.

import asyncio
import json
import os
import re
import string
import warnings
from typing import Any, Dict, Literal, Optional

from dotenv import load_dotenv
from openai import AsyncOpenAI, OpenAI
from pydantic import BaseModel

load_dotenv()

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL")
JUDGE_MODEL_NAME = os.environ.get("JUDGE_MODEL_NAME", "gpt-4.1-2025-04-14")

evaluation_llm_client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
model_as_a_judge_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)


# ================================================
# verify_answer_simpleqa
# ================================================

EVALUATION_PROMPT_SIMPLEQA = """
Your job is to look at a question, a gold target, and a predicted answer, and then assign a grade of either ["CORRECT", "INCORRECT", "NOT_ATTEMPTED"].
First, I will give examples of each grade, and then you will grade a new example.


The following are examples of CORRECT predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia Obama and Sasha Obama
Predicted answer 1: sasha and malia obama
Predicted answer 2: most people would say Malia and Sasha, but I'm not sure and would have to double check
Predicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha Marian, but they are commonly referred to as Malia Obama and Sasha Obama. Malia was born on July 4, 1998, and Sasha was born on June 10, 2001.
```
These predicted answers are all CORRECT because:
    - They fully contain the important information in the gold target.
    - They do not contain any information that contradicts the gold target.
    - Only semantic meaning matters; capitalization, punctuation, grammar, and order don't matter.
    - Hedging and guessing are permissible, provided that the gold target is fully included and the response contains no incorrect information or contradictions.


The following are examples of INCORRECT predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia and Sasha
Predicted answer 1: Malia.
Predicted answer 2: Malia, Sasha, and Susan.
Predicted answer 3: Barack Obama does not have any children.
Predicted answer 4: I think it's either Malia and Sasha. Or it could be Malia and Jackie. Or it could be Joey and Malia.
Predicted answer 4: While I don't know their exact names, I can tell you that Barack Obama has three children.
Predicted answer 5: It's possible you may mean Betsy and Olivia. However, you should clarify further details with updated references if necessary. Is that the correct answer?
Predicted answer 6: It may be the case that Obama's child is named James. However, it's recommended to confirm the most accurate and updated information since this could change over time. This model may not always reflect the most current information.
```
These predicted answers are all INCORRECT because:
    - A factual statement in the answer contradicts the gold target. Incorrect statements that have some hedging (e.g., "it is possible that", "although i'm not sure, i think") are also considered incorrect.


The following are examples of NOT_ATTEMPTED predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia and Sasha
Predicted answer 1: I don't know.
Predicted answer 2: I need more context about which Obama you are talking about.
Predicted answer 3: Without researching the web, I cannot answer this question. However, I can tell you that Barack Obama has two children.
Predicted answer 4: Barack Obama has two children. I know that one of them is Malia, but I'm not sure about the other one.
```
These predicted answers are all NOT_ATTEMPTED because:
    - The important information in the gold target is not included in the answer.
    - No statements in the answer contradict the gold target.


Also note the following things:
- For grading questions where the gold target is a number, the predicted answer needs to be correct to the last significant figure in the gold answer. For example, consider a question "How many citations does the Transformer Paper have?" with gold target "120k". 
    - Predicted answers "120k", "124k", and 115k" are all CORRECT. 
    - Predicted answers "100k" and "113k" are INCORRECT. 
    - Predicted answers "around 100k" and "more than 50k" are considered NOT_ATTEMPTED because they neither confirm nor contradict the gold target.
- The gold target may contain more information than the question. In such cases, the predicted answer only needs to contain the information that is in the question.
    - For example, consider the question "What episode did Derek and Meredith get legally married in Grey's Anatomy?" with gold target "Season 7, Episode 20: White Wedding". Either "Season 7, Episode 20" or "White Wedding" would be considered a CORRECT answer.
- Do not punish predicted answers if they omit information that would be clearly inferred from the question.
    - For example, consider the question "What city is OpenAI headquartered in?" and the gold target "San Francisco, California". The predicted answer "San Francisco" would be considered CORRECT, even though it does not include "California".
    - Consider the question "What award did A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity win at NAACL '24?", the gold target is "Outstanding Paper Award". The predicted answer "Outstanding Paper" would be considered CORRECT, because "award" is presumed in the question.
    - For the question "What is the height of Jason Wei in meters?", the gold target is "1.73 m". The predicted answer "1.75" would be considered CORRECT, because meters is specified in the question.
    - For the question "What is the name of Barack Obama's wife?", the gold target is "Michelle Obama". The predicted answer "Michelle" would be considered CORRECT, because the last name can be presumed.
- Do not punish for typos in people's name if it's clearly the same name. 
    - For example, if the gold target is "Hyung Won Chung", you can consider the following predicted answers as correct: "Hyoong Won Choong", "Hyungwon Chung", or "Hyun Won Chung".


Here is a new example. Simply reply with either CORRECT, INCORRECT, NOT ATTEMPTED. Don't apologize or correct yourself if there was a mistake; we are just trying to grade the answer.
```
Question: {}
Gold target: {}
Predicted answer: {}
```

Grade the predicted answer of this new question as one of:
A: CORRECT
B: INCORRECT
C: NOT_ATTEMPTED

Just return the letters "A", "B", or "C", with no text around it.
""".strip()


async def verify_answer_simpleqa(
    question: str, target: str, predicted_answer: str
) -> str:
    """
    Use LLM to verify if the predicted answer is correct.
    Expects the LLM to choose between A (correct), B or C (incorrect).
    """
    messages = [
        {
            "role": "user",
            "content": EVALUATION_PROMPT_SIMPLEQA.format(
                question, target, predicted_answer
            ),
        }
    ]
    CHOICE_MAP = {"A": "CORRECT", "B": "INCORRECT", "C": "NOT_ATTEMPTED"}

    try:
        llm_response = await evaluation_llm_client.chat.completions.create(
            model=JUDGE_MODEL_NAME, messages=messages, max_completion_tokens=2
        )
        content = llm_response.choices[0].message.content
        match = re.search(r"(A|B|C)", content)
        if match:
            return CHOICE_MAP[match.group(0)]
    except Exception as e:
        print(f"LLM evaluation failed: {e}")

    return "NOT_ATTEMPTED"


# ================================================
# verify_answer_hle
# ================================================

HLE_JUDGE_PROMPT = """Judge whether the following [response] to [question] is correct or not based on the precise and unambiguous [correct_answer] below.

[question]: {question}

[response]: {response}

Your judgement must be in the format and criteria specified below:

extracted_final_answer: The final exact answer extracted from the [response]. Put the extracted answer as 'None' if there is no exact, final answer to extract from the response.

[correct_answer]: {correct_answer}

reasoning: Explain why the extracted_final_answer is correct or incorrect based on [correct_answer], focusing only on if there are meaningful differences between [correct_answer] and the extracted_final_answer. Do not comment on any background to the problem, do not attempt to solve the problem, do not argue for any answer different than [correct_answer], focus only on whether the answers match.

correct: Answer 'yes' if extracted_final_answer matches the [correct_answer] given above, or is within a small margin of error for numerical problems. Answer 'no' otherwise, i.e. if there if there is any inconsistency, ambiguity, non-equivalency, or if the extracted answer is incorrect.

confidence: The extracted confidence score between 0|\%| and 100|\%| from [response]. Put 100 if there is no confidence score available."""


class HLEExtractedAnswer(BaseModel):
    extracted_final_answer: str
    reasoning: str
    correct: Literal["yes", "no"]
    confidence: int
    strict: Literal[True] = True  # 100% reliability


async def verify_answer_hle(question: str, target: str, predicted_answer: str) -> str:
    """
    Use HLE-style LLM judge to verify if the predicted answer is correct.
    Returns the evaluation result as a string: "CORRECT", "INCORRECT", or "NOT_ATTEMPTED".

    Args:
        question: The question being answered
        target: The correct/target answer
        predicted_answer: The model's predicted answer

    Returns:
        String indicating the evaluation result
    """
    prompt = HLE_JUDGE_PROMPT.format(
        question=question, correct_answer=target, response=predicted_answer
    )

    try:
        response = await evaluation_llm_client.beta.chat.completions.parse(
            model=JUDGE_MODEL_NAME,
            max_completion_tokens=4096,
            messages=[{"role": "user", "content": prompt}],
            response_format=HLEExtractedAnswer,
        )

        content = response.choices[0].message.parsed

        # Print HLE reasoning
        print(f"LLM as Judge Reasoning: {content.reasoning}")
        print(f"LLM as Judge Result: {content.correct}")
        print(f"LLM as Judge Confidence: {content.confidence}%")

        # Convert HLE format to eval_utils format
        if content.correct == "yes":
            return "CORRECT"
        else:
            return "INCORRECT"

    except Exception as e:
        if "Incorrect API key provided" in str(e):
            print(f"LLM evaluation failed: {e}")
            exit()
        print(f"LLM evaluation failed: {e}")
        return "NOT_ATTEMPTED"


# ================================================
# verify_answer_gaia
# ================================================


async def verify_answer_gaia(question: str, target: str, predicted_answer: str) -> str:
    """
    Use GAIA-style judge to verify if the predicted answer is correct.
    """

    def normalize_number_str(number_str: str) -> float | None:
        # we replace these common units and commas to allow
        # conversion to float
        for char in ["$", "%", ","]:
            number_str = number_str.replace(char, "")
        try:
            return float(number_str)
        except ValueError:
            print(f"String {number_str} cannot be normalized to number str.")
            return None  # Return None instead of inf to handle gracefully

    def split_string(
        s: str,
        char_list: list[str] = [",", ";"],
    ) -> list[str]:
        pattern = f"[{''.join(char_list)}]"
        return re.split(pattern, s)

    def normalize_str(input_str, remove_punct=True) -> str:
        """
        Normalize a string by:
        - Removing all white spaces
        - Optionally removing punctuation (if remove_punct is True)
        - Converting to lowercase
        Parameters:
        - input_str: str, the string to normalize
        - remove_punct: bool, whether to remove punctuation (default: True)
        Returns:
        - str, the normalized string
        """
        # Remove all white spaces. Required e.g for seagull vs. sea gull
        no_spaces = re.sub(r"\s", "", input_str)

        # Remove punctuation, if specified.
        if remove_punct:
            translator = str.maketrans("", "", string.punctuation)
            return no_spaces.lower().translate(translator)
        else:
            return no_spaces.lower()

    def question_scorer(
        model_answer: str,
        ground_truth: str,
    ) -> bool:
        def is_float(element: any) -> bool:
            try:
                float(element)
                return True
            except ValueError:
                return False

        if model_answer is None:
            model_answer = "None"

        # if gt is a number
        if is_float(ground_truth):
            print(f"Evaluating {model_answer} as a number.")
            normalized_answer = normalize_number_str(model_answer)
            # If normalization failed, the answer is incorrect
            if normalized_answer is None:
                return False
            return normalized_answer == float(ground_truth)

        # if gt is a list
        elif any(char in ground_truth for char in [",", ";"]):
            print(f"Evaluating {model_answer} as a comma separated list.")
            # question with the fish: normalization removes punct

            gt_elems = split_string(ground_truth)
            ma_elems = split_string(model_answer)

            # check length is the same
            if len(gt_elems) != len(ma_elems):
                warnings.warn(
                    "Answer lists have different lengths, returning False.", UserWarning
                )
                return False

            # compare each element as float or str
            comparisons = []
            for ma_elem, gt_elem in zip(ma_elems, gt_elems):
                if is_float(gt_elem):
                    normalized_ma_elem = normalize_number_str(ma_elem)
                    # If normalization failed, this element is incorrect
                    if normalized_ma_elem is None:
                        comparisons.append(False)
                    else:
                        comparisons.append(normalized_ma_elem == float(gt_elem))
                else:
                    # we do not remove punct since comparisons can include punct
                    comparisons.append(
                        normalize_str(ma_elem, remove_punct=False)
                        == normalize_str(gt_elem, remove_punct=False)
                    )
            return all(comparisons)

        # if gt is a str
        else:
            print(f"Evaluating {model_answer} as a string.")
            return normalize_str(model_answer) == normalize_str(ground_truth)

    # Use the question_scorer to evaluate the answer
    try:
        is_correct = question_scorer(predicted_answer, target)
        return "CORRECT" if is_correct else "INCORRECT"
    except Exception as e:
        print(f"GAIA evaluation failed: {e}")
        raise e

        # use raise error instead, later we could judge it as NOT_ATTEMPTED.
        # return "NOT_ATTEMPTED"


# ================================================
# verify_answer_gaia_validation_text_103

# Prompt from WebAgent
# https://github.com/Alibaba-NLP/WebAgent/blob/f25dae54daf0ce2874ffd5ed5ffb20feca7c4c4e/WebSailor/src/prompt.py#L98
# ================================================

GAIA_VALIDATION_TEXT_103_SCORER_PROMPT = """You are an evaluation assistant. Please determine if the predicted answer is equivalent to the labeled answer.

Question: {question}

Labeled Answer: {correct_answer}

Predicted Answer: {response}

Did the model give an answer **equivalent** to the labeled answer? Please respond with "Correct" if they are equivalent, or "Incorrect" if they are not equivalent. Do not include any other text.
"""


async def verify_answer_gaia_validation_text_103(
    question: str, target: str, predicted_answer: str
) -> str:
    prompt = GAIA_VALIDATION_TEXT_103_SCORER_PROMPT.format(
        question=question, correct_answer=target, response=predicted_answer
    )

    max_tries = 10
    for attempt in range(max_tries):
        try:
            response = await evaluation_llm_client.chat.completions.create(
                model=JUDGE_MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
            )

            content = response.choices[0].message.content
            print("LLM Judge Response: ", content)

            if response:
                break
        except Exception as e:
            if attempt == (max_tries - 1):
                raise e

    # Use case-insensitive matching and strip whitespace/punctuation
    content_normalized = content.strip().rstrip(".").lower()
    if content_normalized == "correct":
        return "CORRECT"
    elif content_normalized == "incorrect":
        return "INCORRECT"
    else:
        # If we can't parse the response, default to NOT_ATTEMPTED to trigger retry
        print(f"Warning: Could not parse judge response: {content}")
        return "NOT_ATTEMPTED"


# ================================================
# verify_answer_browsecomp

# Prompt from Tongyi DeepResearch
# https://github.com/Alibaba-NLP/DeepResearch/blob/main/WebAgent/WebWatcher/infer/evaluation/prompt.py#L110
# ================================================

JUDGE_PROMPT_BC_zh = """
请根据给定问题、标准答案和模型预测的答案来评估模型的回答是否正确。您的任务是将结果评定为：【正确】、【错误】。

首先，我们将列出每个评定类别的示例，然后请您对新问题的预测答案进行评定。
以下是【正确】的答复示例：
```
问题：贝拉克·奥巴马的孩子叫什么名字？
标准答案：玛丽亚·奥巴马和萨莎·奥巴马
模型预测1：Malia Obama and Sasha Obama
模型预测2：玛丽亚和萨沙
模型预测3：大多数人会说是玛丽亚和萨莎，但我不确定，需要再确认
模型预测4：巴拉克·奥巴马有两个女儿，她们分别是玛丽亚·安和娜塔莎·玛丽安，但通常称作玛丽亚·奥巴马和萨莎·奥巴马。
```
这些答复均为【正确】，因为：
    - 完整地包含了标准答案中的重要信息。
    - 不包含任何与标准答案矛盾的信息。
    - 只关注语义内容，中英文，大小写、标点、语法和顺序不重要。
    - 答复中出现模糊语句或猜测是可以接受的，前提是包含了标准答案且不含有不正确信息或矛盾。

以下是【错误】的答复示例：
```
问题：巴拉克·奥巴马的孩子叫什么名字？
标准答案：玛丽亚·奥巴马和萨莎·奥巴马
模型预测1：玛丽亚
模型预测2：玛丽亚、萨莎和苏珊和萨莎·奥巴马或玛丽亚·奥巴马，或娜塔莎·玛丽安，或爱因斯坦
模型预测3：虽然我不知道他们的确切名字，但能说出巴拉克·奥巴马有两个孩子。
模型预测4：你可能是想说贝茜和奥利维亚。不过您应通过最新的参考资料确认详细信息。那是正确的答案吗？
模型预测5：巴拉克·奥巴马的孩子
```
这些答复均为【错误】，因为：
    - 答复中包含与标准答案矛盾的事实陈述。
    - 答案为空、重复表述问题。
    - 答案枚举了多个答案，重复表述答案。

需要格外注意的是：
- 标准答案中包含对于问题中多个方面的回答，并且在同一个方面的答案中可能会有多种不同的描述，这些描述均是正确的，并且在同一个括号中给出，通过逗号连接。例如，考虑问题"抖音自己的人工智能大模型叫什么名字？"，标准答案为"【【豆包，云雀】】"：
    - 预测答案"豆包"、"豆包、云雀"、"云雀"等均为【正确】。
- 对于标准答案中包含的不同方面的回答，模型需要同时给出所有方面的回答才可以算是正确，否则直接判断为【错误】，不存在【部分正确】这种输出方式，这些答案会在不同的括号中给出。例如，考虑问题"TFBOYS组合中的成员有哪些？"，标准答案为"【【王俊凯】【王源】【易洋千玺】】"：
    - 预测答案"王俊凯、王源、易洋千玺"等同时包含所有答案，才可以算为【正确】。
    - 预测答案为"王俊凯、易洋千玺"等没有同时包含所有答案，会被算为【错误】。

另外注意以下几点：
- 对于标准答案为数字的问题，预测答案应和标准答案一致。例如，考虑问题"金山铁路黄浦江特大桥的全长是多少米？"，标准答案为"3518.17"：
    - 预测答案"3518"、"3518.1"、"3518.17"均为【正确】。
    - 预测答案"3520"和"3600"均为【错误】。
- 如果模型预测并没有直接回答问题，模型试图绕过或未能直接给出标准答案视为【错误】答案。
    - 例如：问题"林宥嘉的老婆是谁"，标准答案为"丁文琪"。模型预测"林宥嘉的老婆"、"林宥嘉的老婆应该很优秀"、"林宥嘉的老婆可能是某个公众人物"均为【错误】。
- 如果标准答案包含比问题更多的信息，预测答案只需包含问题中提到的信息。
    - 例如，考虑问题"菱镁矿的主要化学成分是什么？"标准答案为"碳酸镁（MgCO3）"。"碳酸镁"或"MgCO3"均视为【正确】答案。
- 如果从问题中明显可以推断出预测答案省略的信息，那么算作正确。
    - 例如，问题"巴鲁米尼的努拉吉遗迹在1997年被联合国教科文组织列为世界文化遗产，那么这遗址在哪个地区？"标准答案为"意大利撒丁岛"，预测答案"撒丁岛"被视为【正确】。
- 如果能明显看出名字翻译版本不同但是是同一个人也认为正确。
    - 例如，如果标准答案是"Robinson"，那么回答鲁滨逊或者鲁滨孙均正确。
- 你应该更关注标准答案和模型预测的匹配度，而不是关心标准答案是否是正确的。

下面是一个新的问题示例。请只回复【正确】、【错误】之一，不要道歉或纠正自己的错误，只需要评估该回答。
```
问题: {question}
标准答案: {correct_answer}
预测答案: {response}
```

将此新问题的预测答案评定为以下之一：
A.【正确】
B.【错误】

只返回【正确】、【错误】所代表的选项即可，即仅返回A或B即可，无须添加任何其他的文本。
""".strip()


JUDGE_PROMPT_BC_en = """
Based on the given question, standard answer, and model-predicted answer, evaluate whether the model's response is correct. Your task is to classify the result as: [CORRECT] or [INCORRECT].

First, we'll list examples for each category, then you'll evaluate a new question's predicted answer.
Here are examples of [CORRECT] responses:
```
Question: What are the names of Barack Obama's children?
Standard Answer: Malia Obama and Sasha Obama
Model Prediction 1: Malia Obama and Sasha Obama
Model Prediction 2: Malia and Sasha
Model Prediction 3: Most would say Malia and Sasha, but I'm not sure, I should verify
Model Prediction 4: Barack Obama has two daughters, Malia Ann and Natasha Marian, commonly known as Malia Obama and Sasha Obama.
```
These responses are all [CORRECT] because they:
    - Fully include the important information from the standard answer.
    - Don't contain any information that contradicts the standard answer.
    - Focus only on semantic content; language, capitalization, punctuation, grammar, and order aren't important.
    - Vague statements or guesses are acceptable as long as they include the standard answer and don't contain incorrect information or contradictions.

Here are examples of [INCORRECT] responses:
```
Question: What are the names of Barack Obama's children?
Standard Answer: Malia Obama and Sasha Obama
Model Prediction 1: Malia
Model Prediction 2: Malia, Sasha and Susan or Sasha Obama or Malia Obama, or Natasha Marian, or Einstein
Model Prediction 3: While I don't know their exact names, I can tell you Barack Obama has two children.
Model Prediction 4: You might be thinking of Betsy and Olivia. But you should verify the details with the latest references. Is that the correct answer?
Model Prediction 5: Barack Obama's children
```
These responses are all [INCORRECT] because they:
    - Contain factual statements that contradict the standard answer.
    - Are empty or merely repeat the question.
    - Enumerate multiple answers or repeat the answer.

Pay special attention to the following:
- The standard answer may contain responses to multiple aspects of the question, and within the same aspect, there might be different descriptions, all of which are correct and are given in the same bracket, connected by commas. For example, for the question "What is the name of ByteDance's AI model?", the standard answer is "[[Doubao, Skylark]]":
    - Predicted answers "Doubao", "Doubao, Skylark", "Skylark", etc. are all [CORRECT].
- For standard answers containing responses to different aspects, the model needs to provide answers to all aspects to be considered correct; otherwise, it's directly judged as [INCORRECT]. There is no [PARTIALLY CORRECT] output option. These answers will be given in different brackets. For example, for the question "Who are the members of TFBOYS?", the standard answer is "[[Wang Junkai][Wang Yuan][Yi Yangqianxi]]":
    - Predicted answers like "Wang Junkai, Wang Yuan, Yi Yangqianxi" that include all answers are [CORRECT].
    - Predicted answers like "Wang Junkai, Yi Yangqianxi" that don't include all answers are [INCORRECT].

Also note the following points:
- For questions with numerical standard answers, the predicted answer should match the standard answer. For example, for the question "What is the total length in meters of the Huangpu River Bridge on the Jinshan Railway?", the standard answer is "3518.17":
    - Predicted answers "3518", "3518.1", "3518.17" are all [CORRECT].
    - Predicted answers "3520" and "3600" are [INCORRECT].
- If the model prediction doesn't directly answer the question, attempts to circumvent or fails to directly provide the standard answer, it's considered an [INCORRECT] answer.
    - For example, for the question "Who is JJ Lin's wife?", with the standard answer "Ding Wenqi", model predictions like "JJ Lin's wife", "JJ Lin's wife should be excellent", "JJ Lin's wife might be a public figure" are all [INCORRECT].
- If the standard answer contains more information than the question asks for, the predicted answer only needs to include the information mentioned in the question.
    - For example, for the question "What is the main chemical component of magnesite?", with the standard answer "Magnesium carbonate (MgCO3)", "Magnesium carbonate" or "MgCO3" are both considered [CORRECT] answers.
- If information omitted in the predicted answer can be clearly inferred from the question, it's considered correct.
    - For example, for the question "The Nuragic ruins of Barumini were listed as a World Cultural Heritage by UNESCO in 1997, so where is this site located?", with the standard answer "Sardinia, Italy", the predicted answer "Sardinia" is considered [CORRECT].
- If it's clear that different translations of a name refer to the same person, it's considered correct.
    - For example, if the standard answer is "Robinson", answers like "Lubinson" or "Lubinsun" are both correct.
- You should focus more on the match between the standard answer and the model prediction, rather than whether the standard answer itself is correct.

Below is a new question example. Please reply with only [CORRECT] or [INCORRECT], without apologies or corrections to your own errors, just evaluate the answer.
```
Question: {question}
Standard Answer: {correct_answer}
Predicted Answer: {response}
```

Evaluate this new question's predicted answer as one of the following:
A. [CORRECT]
B. [INCORRECT]

Return only the option representing [CORRECT] or [INCORRECT], i.e., just return A or B, without adding any other text.
""".strip()


async def verify_answer_browsecomp(
    question: str, target: str, predicted_answer: str
) -> str:
    """
    Use BrowseComp judge (English version) to verify if the predicted answer is correct.
    Expects the LLM to return A (correct) or B (incorrect).
    """

    prompt = JUDGE_PROMPT_BC_en.format(
        question=question, correct_answer=target, response=predicted_answer
    )

    try:
        response = await evaluation_llm_client.chat.completions.create(
            model=JUDGE_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=2,
        )

        content = response.choices[0].message.content
        print(f"BrowseComp Judge Response: {content}")

        # Extract A or B from the response
        match = re.search(r"[AB]", content)
        if match:
            choice = match.group(0)
            if choice == "A":
                return "CORRECT"
            elif choice == "B":
                return "INCORRECT"

        # If no clear A or B is found, return NOT_ATTEMPTED to trigger retry
        print(f"Warning: Could not parse BrowseComp judge response: {content}")
        return "NOT_ATTEMPTED"

    except Exception as e:
        print(f"BrowseComp evaluation failed: {e}")
        raise e


async def verify_answer_browsecomp_zh(
    question: str, target: str, predicted_answer: str
) -> str:
    """
    Use BrowseComp judge (Chinese version) to verify if the predicted answer is correct.
    Expects the LLM to return A (correct) or B (incorrect).
    """

    prompt = JUDGE_PROMPT_BC_zh.format(
        question=question, correct_answer=target, response=predicted_answer
    )

    try:
        response = await evaluation_llm_client.chat.completions.create(
            model=JUDGE_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_completion_tokens=2,
        )

        content = response.choices[0].message.content
        print(f"BrowseComp-ZH Judge Response: {content}")

        # Extract A or B from the response
        match = re.search(r"[AB]", content)
        if match:
            choice = match.group(0)
            if choice == "A":
                return "CORRECT"
            elif choice == "B":
                return "INCORRECT"

        # If no clear A or B is found, return NOT_ATTEMPTED to trigger retry
        print(f"Warning: Could not parse BrowseComp-ZH judge response: {content}")
        return "NOT_ATTEMPTED"

    except Exception as e:
        print(f"BrowseComp-ZH evaluation failed: {e}")
        raise e


# ================================================
# verify_answer_xbench_deepsearch

# Prompt from XBench-Evals
# https://github.com/xbench-ai/xbench-evals/blob/main/eval_grader.py#L25
# ================================================

JUDGE_PROMPT_XBENCH = """
你是一个通用人工智能助手。根据下面给出的[正确答案], 判断以下对[原问题]的[回答]的回答是否正确。

[原问题]: {question}

[正确答案]: {correct_answer}

[回答]:{response}

你的判断必须按照以下格式和标准进行:

最终答案: 从[回答]中提取出的最终准确答案。如果[回答]中没有明确的最终答案, 则填写'无'。

解释: 根据[正确答案]解释为什么[最终答案]是正确的或错误的。只关注[最终答案]与[正确答案]之间是否存在实质性差异, 不要评论题目的背景, 不要尝试重新解题, 不要为任何不同于[正确答案]的答案辩护, 只专注于判断答案是否一致。

结论: 如果[最终答案]与上方给出的[正确答案]一致, 或者在数值题目中处于可接受的微小误差范围内, 则填写'正确'; 否则（即存在任何不一致、歧义、不等价或提取出的答案错误的情况）填写'错误'。
""".strip()


async def verify_answer_xbench_deepsearch(
    question: str, target: str, predicted_answer: str
) -> str:
    """
    Use XBench-DeepSearch judge to verify if the predicted answer is correct.
    """

    def parse_match_result(match):
        if match is None:
            return match
        match = match.group(0)
        try:
            target = match.split(":")[1].strip()
            return target
        except Exception:
            return match  # return naive result in case of failed

    if predicted_answer is None:
        return "INCORRECT"

    judge_prompt = JUDGE_PROMPT_XBENCH.format(
        question=question,
        correct_answer=target,
        response=predicted_answer,
    )
    try:
        response = await evaluation_llm_client.chat.completions.create(
            model=JUDGE_MODEL_NAME,
            messages=[{"role": "user", "content": judge_prompt}],
        )
        judge_response = response.choices[0].message.content
    except Exception:
        judge_response = None
    if judge_response is None:
        return "NOT_ATTEMPTED"

    # Extract grader conclusions
    extract_match = re.search(r"最终答案:*(.*)", judge_response)
    extract_match = parse_match_result(extract_match)

    # Fixed regex: make the dot optional with \s* (zero or more whitespace)
    correct_match = re.search(r"结论:*\s*(正确|错误)", judge_response)
    correct_match = parse_match_result(correct_match)

    explain_match = re.search(r"解释:*(.*)", judge_response)
    explain_match = parse_match_result(explain_match)

    # Print debug info
    print(f"XBench Judge - Extract: {extract_match}, Correct: {correct_match}")

    if correct_match == "正确":
        return "CORRECT"
    elif correct_match == "错误":
        return "INCORRECT"
    else:
        # If we can't parse the result, return NOT_ATTEMPTED to trigger retry
        print(
            f"Warning: Could not parse XBench judge response, correct_match={correct_match}"
        )
        return "NOT_ATTEMPTED"


# ================================================
# verify_answer_deepsearchqa
#
# Official prompt from DeepSearchQA benchmark
# https://www.kaggle.com/code/andrewmingwang/deepsearchqa-starter-code
# ================================================

JUDGE_PROMPT_DEEPSEARCHQA = """Your task is to evaluate whether a given "AI Response" for a specific "User Prompt" arrived at the correct answer.

**Answer Correctness Task**

*   **Purpose:** Assess whether the AI response provides the correct answer(s) based on the provided "Correct Answer" and "Prompt Type".
*   **Process:**
    *   Identify the "Prompt Type": "<prompt_type>".
    *   Refer to the "Correct Answer": "<answer>".
    *   Based on the "Prompt Type", determine if the "AI Response" contains the expected answer(s).
        *   **'Single Answer'**: Check if the response provides the answer that addresses the user's question. It does not have to match the exact wording of the provided answer.
        *   **'Set Answer'**: Check if the response includes *each* item from the provided ground truth answers. The order might not matter unless specified otherwise. The response might include more answers than the list. Determine the correctness *only* based on the list first and then check if the response includes answers not in the list.
    *   **Explanation:** Provide a brief explanation justifying your assessment of answer correctness, referencing specific parts of the AI response and the correct answer.
    *   **Correctness Details:** Provide a dictionary, one key for each expected answer part, and value is a boolean indicating whether each expected answer part was found.
        *   For 'Set Answer', this will be a list of attributes, one for each item/part in the "Correct Answer". Each key will be a string indicating the expected answer part, and the value will be a boolean indicating whether that part was found in the response.
    *   **Excessive Answers:** Provide a list of strings, each indicating an excessive answer part. If the response provides answers that are **not** in the "Correct Answer" list, add these answers as excessive answers. Return an empty list when there's no excessive answers in the response.


**Output Format:**

Your evaluation *must* be structured as a nested JSON dictionary with the following top-level keys: `"Answer Correctness"`. Please return NULL if any of "Prompt", "AI Response" or "Correct Answer" is empty.
The value for `"Answer Correctness"` should be a dictionary containing `"Explanation"` (a string), `"Correctness Details"` (a dictionary where each key is the expected correct answer, and the value is a boolean indicating whether the response contains the correct answer), and `"Excessive Answers"` (a list of strings indicating the excessive answers).

Make sure you return a valid JSON string. Pay special attention to quotes, commas and special characters in the JSON string. Make sure to escape all special characters and quotes in the JSON string.


**Example (Partial):**

"```json
{{
  "Answer Correctness": {{
    "Explanation": "The response correctly identified Belgium and France but also includes an excessive answer, Italy.",
    "Correctness Details": {{
      "Belgium": true,
      "France": true,
    }},
    "Excessive Answers": [ "Italy" ]
  }}
}}
```"

**Now, proceed with the evaluation using the provided User Prompt, AI Response, and Correct Answer.**

User Prompt (Wrapped in <prompt> and </prompt>):
<prompt>
{prompt}
</prompt>
--------------------
**  Correct Answer (Wrapped in <answer> and </answer>):
Prompt Type: {prompt_type}
<answer>
{answer}
</answer>
--------------------
AI assistant response (Wrapped in <response> and </response>):
<response>
{response}
</response>

--------------------
Rating:"""


async def verify_answer_deepsearchqa(
    question: str,
    target: str,
    predicted_answer: str,
    metadata: Optional[Dict[str, Any]] = None,
) -> tuple[str, str, Optional[Dict[str, Any]]]:
    """
    Use DeepSearchQA-specific judge to verify if the predicted answer is correct.
    Uses the official DeepSearchQA evaluation prompt with JSON output format.

    Args:
        question: The question being answered
        target: The correct/target answer
        predicted_answer: The model's predicted answer
        metadata: Optional metadata dict with additional context (e.g., problem_category, answer_type)

    Returns:
        Tuple of (result, judge_type, details_dict):
        - result: "CORRECT", "INCORRECT", or "NOT_ATTEMPTED"
        - judge_type: "deepsearchqa_judge"
        - details_dict: Dict with keys:
            - correctness_details: Dict[str, bool] mapping answer parts to correctness
            - excessive_answers: List[str] of extra answers not in ground truth
            - explanation: str explaining the judgment
            - num_correct: int number of correct answer parts
            - num_expected: int total number of expected answer parts
            - num_excessive: int number of excessive answers
    """

    if predicted_answer is None:
        return "INCORRECT", "deepsearchqa_judge", None

    # Determine prompt_type from metadata
    prompt_type = "Single Answer"  # Default
    if metadata and "answer_type" in metadata:
        answer_type = metadata["answer_type"]
        # Map answer_type to prompt_type
        if answer_type == "Set Answer":
            prompt_type = "Set Answer"
        # Add more mappings if needed

    judge_prompt = JUDGE_PROMPT_DEEPSEARCHQA.format(
        prompt_type=prompt_type,
        prompt=question,
        answer=target,
        response=predicted_answer,
    )

    try:
        response = await evaluation_llm_client.chat.completions.create(
            model=JUDGE_MODEL_NAME,
            messages=[{"role": "user", "content": judge_prompt}],
        )
        judge_response = response.choices[0].message.content
    except Exception as e:
        print(f"DeepSearchQA judge failed: {e}")
        return "NOT_ATTEMPTED", "deepsearchqa_judge", None

    if judge_response is None:
        return "NOT_ATTEMPTED", "deepsearchqa_judge", None

    # Parse JSON response
    try:
        # Extract JSON from the response (might be wrapped in markdown code blocks)
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", judge_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find JSON without code blocks
            json_match = re.search(r"\{.*\}", judge_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                print("Warning: Could not find JSON in DeepSearchQA judge response")
                return "NOT_ATTEMPTED", "deepsearchqa_judge", None

        result = json.loads(json_str)
        answer_correctness = result.get("Answer Correctness", {})

        explanation = answer_correctness.get("Explanation", "")
        correctness_details = answer_correctness.get("Correctness Details", {})
        excessive_answers = answer_correctness.get("Excessive Answers", [])

        # Calculate statistics
        num_expected = len(correctness_details)
        num_correct = sum(1 for v in correctness_details.values() if v)
        num_excessive = len(excessive_answers)

        # Build details dict
        details = {
            "correctness_details": correctness_details,
            "excessive_answers": excessive_answers,
            "explanation": explanation,
            "num_correct": num_correct,
            "num_expected": num_expected,
            "num_excessive": num_excessive,
        }

        # Print debug info
        print(
            f"DeepSearchQA Judge - Correct: {num_correct}/{num_expected}, Excessive: {num_excessive}"
        )
        print(f"DeepSearchQA Judge - Explanation: {explanation}")

        # Determine if answer is correct
        # Following official logic: all expected parts must be found, and no excessive answers
        if correctness_details:
            all_correct = all(correctness_details.values())
            if all_correct and not excessive_answers:
                return "CORRECT", "deepsearchqa_judge", details
            else:
                # Either missing some expected answers or has excessive answers
                return "INCORRECT", "deepsearchqa_judge", details
        else:
            # No correctness details, can't determine
            return "NOT_ATTEMPTED", "deepsearchqa_judge", None

    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse JSON from DeepSearchQA judge: {e}")
        print(f"Response: {judge_response[:200]}...")
        return "NOT_ATTEMPTED", "deepsearchqa_judge", None
    except Exception as e:
        print(f"Warning: Error processing DeepSearchQA judge response: {e}")
        return "NOT_ATTEMPTED", "deepsearchqa_judge", None


# ================================================
# verify_answer_for_datasets
# ================================================


async def _verify_answer_for_datasets_core(
    benchmark_name: str,
    question: str,
    target: str,
    predicted_answer: str,
    metadata: Optional[Dict[str, Any]] = None,
) -> tuple[str, str, Optional[Dict[str, Any]]]:
    """
    Verify the answer for a given dataset.

    Args:
        benchmark_name: Name of the benchmark dataset
        question: The question being answered
        target: The correct/target answer
        predicted_answer: The model's predicted answer
        metadata: Optional metadata dict with additional context

    Returns:
        A tuple of (result, judge_type, details_dict).
        details_dict is None for most benchmarks, but contains evaluation details for DeepSearchQA.
    """

    # For benchmarks that need detailed evaluation, don't use exact_match
    if benchmark_name not in ["deepsearchqa"]:
        if predicted_answer == target:
            return "CORRECT", "exact_match", None

    # For gaia-validation, use gaia-validation-text-103-scorer
    # We found that gaia_scorer tends to label many correct answers as incorrect, so we believe
    # that using an LLM-as-judge approach can more accurately reflect the model’s performance.
    if benchmark_name == "gaia-validation":
        # result = await verify_answer_gaia(question, target, predicted_answer)
        # return result, "gaia_scorer", None
        result = await verify_answer_gaia_validation_text_103(
            question, target, predicted_answer
        )
        return result, "gaia_validation_text_103_judge", None

    # For gaia-validation-text-103, use gaia-validation-text-103-scorer
    elif benchmark_name == "gaia-validation-text-103":
        result = await verify_answer_gaia_validation_text_103(
            question, target, predicted_answer
        )
        return result, "gaia_validation_text_103_judge", None

    # For browsecomp (English) and browsecomp-zh (Chinese), use different judges
    elif benchmark_name == "browsecomp":
        result = await verify_answer_browsecomp(question, target, predicted_answer)
        return result, "browsecomp_judge", None

    elif benchmark_name == "browsecomp_zh":
        result = await verify_answer_browsecomp_zh(question, target, predicted_answer)
        return result, "browsecomp_zh_judge", None

    # For hle, hle-text-500, and hle-text-2158, use hle_judge
    elif "hle" in benchmark_name:
        result = await verify_answer_hle(question, target, predicted_answer)
        return result, "hle_judge", None

    # For webwalkerqa, frames, and seal-0, use gaia_validation_text_103_judge
    elif benchmark_name in ["webwalkerqa", "frames", "seal-0"]:
        result = await verify_answer_gaia_validation_text_103(
            question, target, predicted_answer
        )
        return result, "gaia_validation_text_103_judge", None

    # For simpleqa, use simpleqa_judge
    elif benchmark_name == "simpleqa" or benchmark_name == "collect_trace":
        result = await verify_answer_simpleqa(question, target, predicted_answer)
        return result, "simpleqa_judge", None

    # For xbench_deepsearch, use xbench_deepsearch_judge
    elif benchmark_name == "xbench_deepsearch":
        result = await verify_answer_xbench_deepsearch(
            question, target, predicted_answer
        )
        return result, "xbench_deepsearch_judge", None

    # For deepsearchqa, use deepsearchqa_judge (with metadata support and detailed evaluation)
    elif benchmark_name == "deepsearchqa":
        result, judge_type, details = await verify_answer_deepsearchqa(
            question, target, predicted_answer, metadata
        )
        # Return details for DeepSearchQA-specific metrics calculation
        return result, judge_type, details

    # For other benchmarks, use gaia_validation_text_103_judge
    else:
        result = await verify_answer_gaia_validation_text_103(
            question, target, predicted_answer
        )
        return result, "gaia_validation_text_103_judge", None


async def verify_answer_for_datasets(
    benchmark_name: str,
    question: str,
    target: str,
    predicted_answer: str,
    metadata: Optional[Dict[str, Any]] = None,
    max_retries: int = 10,
    retry_interval: int = 5,
) -> tuple[str, str, Optional[Dict[str, Any]]]:
    """
    Wrapper with retry logic for NOT_ATTEMPTED results.

    Args:
        benchmark_name: Name of the benchmark dataset
        question: The question being answered
        target: The correct/target answer
        predicted_answer: The model's predicted answer
        metadata: Optional metadata dict with additional context
        max_retries: Maximum number of retry attempts
        retry_interval: Seconds to wait between retries

    Returns:
        A tuple of (result, judge_type, details_dict).
        details_dict contains evaluation details (for DeepSearchQA) or None (for other benchmarks).
    """
    for attempt in range(1, max_retries + 1):
        result, judge_type, details = await _verify_answer_for_datasets_core(
            benchmark_name, question, target, predicted_answer, metadata
        )
        if result != "NOT_ATTEMPTED":
            return result, judge_type, details
        if attempt < max_retries:
            print(
                f"[Retry {attempt}/{max_retries}] Got NOT_ATTEMPTED, retrying in {retry_interval}s..."
            )
            await asyncio.sleep(retry_interval)

    # still NOT_ATTEMPTED after retries
    print(f"All {max_retries} attempts resulted in NOT_ATTEMPTED.")
    return "NOT_ATTEMPTED", "retry_wrapper", None
